# -*- coding: utf-8 -*-
"""trivago_assessment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c-oi1Et9DIpOQXVAzv5Lc9R4LW7MLhMg

# Imports Modules
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPRegressor
import warnings
warnings.filterwarnings("ignore")

"""## Import Data Set"""

from google.colab import drive
drive.mount('/content/drive/')

import os
os.chdir('/content/drive/My Drive/assesment')

"""## Reading Data"""

data = pd.read_csv('Data.csv', sep=';')
data.head()

"""## Data analysis

We check for null values to gain knowledge from the data set
"""

data.isnull().sum(axis=0)

"""We find out that 'path_id_set' has null values. But we also noticed that 'session_durantion' and 'hits' contain '\N' values, this can cause a problem then we need to fix this."""

# set proper datatypes
data['row_num'] = data['row_num'].astype(np.int)
data['locale'] = data['locale'].astype(str)
data['day_of_week'] = data['day_of_week'].astype(str)
data['hour_of_day'] = data['hour_of_day'].astype(np.int)
data['agent_id'] = data['agent_id'].astype(np.int)
data['entry_page'] = data['entry_page'].astype(np.int)
data.loc[data['path_id_set'].notnull(), 'path_id_set'] = data.loc[data['path_id_set'].notnull(), 'path_id_set'].astype(str)
data['traffic_type'] = data['traffic_type'].astype(str)

# 'session_durantion' and 'hits' contain '\N' values
# we will replace it with 0 for 'session_durantion' and NULL for 'hits'
data['session_durantion'] = pd.to_numeric(data['session_durantion'], errors='coerce')
data['session_durantion'] = data['session_durantion'].fillna(0)
data['hits'] = pd.to_numeric(data['hits'], errors='coerce')

"""## Values of the main features

We check each column to see how to treat them as categorical or continuous

### The platform of the session
"""

locales = data['locale'].value_counts().sort_index()

# show plot
plt.bar(locales.index, locales.values)
plt.show()

"""There are 6 platforms of the session and we will treat them as categorical variable.

### The day of the week of the session
"""

day_of_week = data['day_of_week'].value_counts()[['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]

# show plot
labels = [day[:3] for day in day_of_week.index] # display just first 3 characters of the day of the week
plt.bar(labels, day_of_week.values)
plt.show()

"""There are also 7 days of the week of the session. We will also treat it as a categorical variable.

### The hour of the day of the session
"""

hour_of_day = data['hour_of_day'].value_counts().sort_index()
hour_of_day
# show plot
plt.bar(hour_of_day.index, hour_of_day.values)
plt.xticks(np.arange(24)) # set x ticks to be for each hour: np.arange(24) = [1,2,3,..,23]
plt.show()

"""There are 24 hours. They can be treated as categorical values or continuous values. Because there is a natural ordering of the values, we will treat this variable as continuous.

### The device used for the session
"""

agent_id = data['agent_id'].value_counts().sort_index()

# show plot
plt.bar(agent_id.index, agent_id.values)
plt.show()

"""There are 16 devices used for the session. We will treat this variable as categorical."""

agent_id = data['agent_id'].value_counts().sort_index()
agent_id

"""### Entry page

There are 147 entry pages in total. We will keep the most frequent entry pages (min. 2000 records to keep the page).
"""

pd.set_option('display.max_rows', 10)
data['entry_page'].value_counts()

entry_page = data['entry_page'].value_counts().sort_index()
entry_page

entry_page = data['entry_page'].value_counts().sort_index()
print(entry_page[entry_page > 2000])

keep_entry_pages = entry_page[entry_page > 2000].index
data.loc[data['entry_page'].isin(keep_entry_pages) == False, 'entry_page'] = -1
data['entry_page'].value_counts().sort_index()

"""There are 11 entry pages, which have more than 2000 records, so we will keep them and treat as categories. All the others will belong to category '-1'.

## Locations(path_id_set)

We also have the information about all the locations, which were visited during
the session. There would be too many categories, if we treated this data as categorical, which we cannot afford and train. But this data can still be useful for us. We can create a new variable, which can count the number of pages visited during the session and named 'n_pages_visited'.
"""

data.loc[data['path_id_set'].notnull(), 'n_pages_visited'] = \
data.loc[data['path_id_set'].notnull(), 'path_id_set'].apply(lambda x: len(x.split(';')))

data.loc[data['path_id_set'].isnull() & (data['session_durantion'] > 0), 'n_pages_visited'] = 1
data.loc[data['path_id_set'].isnull() & (data['session_durantion'] == 0), 'n_pages_visited'] = 0

"""## traffic_type

There are 7 categories in traffic type. We treat this variable as categorical.
"""

traffic_type = data['traffic_type'].value_counts().sort_index()

# show plot
plt.bar(traffic_type.index, traffic_type.values)
plt.show()

"""# Prepare data for training

#### We are going to perform several preprocessing steps:

1) Remove locations variable ('path_id_set'), because we do not need it anymore.<br>
2) Perform <b>one-hot-encoding</b> for categorical variables.<br>
3) There are a few extreme values of a target variable ('hits'), which we will replace by 342. Extreme values occur rarely, are difficult to predict and can cause problems while training, so we will focus on predicting casual occations (99.8%).
"""

# remove 'path_id_set'
data = data.drop('path_id_set', axis=1)

# one-hot-encoding
data = pd.get_dummies(data, columns=['locale', 'day_of_week', 'agent_id', 'entry_page', 'traffic_type'], drop_first=True)

hits_x=data[data['hits'].notnull()]['hits']
print("arr : ",hits_x)  
print("50th percentile of arr : ",  
       np.percentile(hits_x, 50)) 
print("25th percentile of arr : ", 
       np.percentile(hits_x, 25)) 
print("99.7th percentile of arr : ", 
       np.percentile(hits_x, 99.7))

hits_x.hist(bins=80)
plt.show()

# to avoid bias we will replace extreme values
n_replaced = (data['hits'] > 284).sum()
print(f'{n_replaced}/{len(data)} will be replaced')

data.loc[data['hits'] > 284, 'hits'] = 284

train = data[data['hits'].notnull()]
pred = data[data['hits'].isnull()]

train_X = train.drop(['row_num', 'hits'], axis=1)
train_y = train['hits']

pred_X = pred.drop(['row_num', 'hits'], axis=1)
pred_row_num = pred['row_num']

train_y.hist(bins=80)
plt.show()

X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.25, random_state=7)

"""# Modelling

#### We are going to test 3 different models:
* Random Forest Regressor
* Linear Regression
* Multilayer Perceptron(MLP)


#### We split the data we have into several parts:

1) Data for prediction - data which have missing values in 'hits' column will be used for prediction<br>
2) Training-validating part:75%-of the randomly selected data, which have non-missing values in ‘hits’ columns, will be used for training using Cross-validation technique<br>
3) Testing part 25% of the randomly selected data, which have non-missing values in ‘hits’ columns, will be used for testing the models<br>
4) 5-Fold Cross Validation is also used to test and validate the model. <br>

#### Objective function

To find the best set of parameters and the best model we will use mean squared error.<br>


#### Error histogram
We will also evaluate our models visually using error histogram. Error histogram shows how much the predicted values deviate from the real values.

## Random Forest Regressor
"""

parameters = {'n_estimators':[15, 20, 25],
              'max_depth': [5, 10, 15],
              'max_features': [5, 10, 15]}

model_rf = GridSearchCV(RandomForestRegressor(), 
                        parameters, 
                        cv=5, 
                        verbose=True,
                        scoring='neg_mean_squared_error',
                        n_jobs=3)
model_rf.fit(X_train, y_train)

best_model_rf = model_rf.best_estimator_
preds_rf = best_model_rf.predict(X_test)
test_mse_rf = mean_squared_error(preds_rf, y_test)

print(f'Best params: {model_rf.best_params_}\n'
      f'Best score: {model_rf.best_score_}\n'
      f'Test mse: {test_mse_rf}')

print(' === ERROR HISTOGRAM ===')
difs_rf = abs(preds_rf - y_test)
plt.hist(difs_rf[difs_rf < 50], bins=100)
plt.show()

"""## Linear Regression"""

parameters = {}

model_lr = GridSearchCV(LinearRegression(), 
                        parameters, 
                        cv=5,
                        verbose=True, 
                        scoring='neg_mean_squared_error', 
                        n_jobs=3)
model_lr.fit(X_train, y_train)

best_model_lr = model_lr.best_estimator_
preds_lr = best_model_lr.predict(X_test)
test_mse_lr = mean_squared_error(preds_lr, y_test)

print(f'Best params: {model_lr.best_params_}\n'
      f'Best score: {model_lr.best_score_}\n'
      f'Test mse: {test_mse_lr}')

print(' === ERROR HISTOGRAM ===')
difs_lr = abs(preds_lr - y_test)
plt.hist(difs_lr[difs_lr < 50], bins=100)
plt.show()

"""## MLP"""

parameters = {'hidden_layer_sizes': [(5), (10), (5, 10), (10, 5), (10, 10)],
              'activation': ['relu']}

model_mlp = GridSearchCV(MLPRegressor(), 
                         parameters, 
                         cv=5, 
                         verbose=True, 
                         scoring='neg_mean_squared_error',
                         n_jobs=3)
model_mlp.fit(X_train, y_train)

best_model_mlp = model_mlp.best_estimator_
preds_mlp = best_model_mlp.predict(X_test)
test_mse_mlp = mean_squared_error(preds_mlp, y_test)

print(f'Best params: {model_mlp.best_params_}\n'
      f'Best score: {model_mlp.best_score_}\n'
      f'Test mse: {test_mse_mlp}')

print(' === ERROR HISTOGRAM ===')
difs_mlp = abs(preds_mlp - y_test)
plt.hist(difs_mlp[difs_mlp < 50], bins=100)
plt.show()

"""# Results

The performance on the test set among the 3 trained models is the following:<br>
* Random Forest Regressor: mse=516,88
* Linear Regression: mse=644,05
* MLP: mse=522,34

Random Forest Regressor has shown the best performance during cross validation and testing, so we will choose it as our final model for predictions.

# Prediction
"""

predictions = best_model_rf.predict(pred_X).astype(np.int)
predictions

predictions = best_model_rf.predict(pred_X).astype(np.int)
predictions_df = pd.DataFrame({'id': pred_row_num, 'hits_prediction': predictions})
predictions_df.to_csv('predictions.csv', index=False, sep=';')